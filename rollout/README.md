# Rollout

**Multi-Turn Tool Call Annotation Framework** â€” an automated multi-turn tool-call data annotation framework.

Rollout is a framework for generating high-quality multi-turn tool-call dialogue data, supporting both single-domain (Single Domain) and cross-domain (Cross Domain) scenarios.

For the Chinese documentation, see [`README_zh.md`](README_zh.md).

## âœ¨ Features

- ðŸ”§ **Unified LLM API** â€” based on LiteLLM, supports 100+ model providers (OpenAI, Anthropic, DeepSeek, etc.)
- ðŸ”„ **Multi-turn dialogues** â€” Agent + User Simulator generate dialogues automatically
- ðŸ› ï¸ **MCP Server integration** â€” supports mocking MCP Server tools
- ðŸ“ **Single-domain / Cross-domain** â€” flexible support for one domain or multi-domain combinations
- âš¡ **Parallel processing** â€” multi-threaded batch processing for higher throughput
- ðŸ’¾ **Resume from checkpoints** â€” checkpoint mechanism supports recovery after interruption
- ðŸŽ¯ **Dialogue monitoring** â€” detects stop conditions to avoid meaningless turns

## ðŸ“ Project structure

```
rollout/
â”œâ”€â”€ run.py                     # manual run entrypoint
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ example_new.yml        # example config
â”‚   â””â”€â”€ models.yml             # custom model registration config
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ batch_rollout_single.py   # single-domain batch script
â”‚   â””â”€â”€ batch_rollout_cross.py    # cross-domain batch script
â”œâ”€â”€ rollout/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ agent.py           # Agent implementation (LiteLLM)
â”‚   â”‚   â”œâ”€â”€ user.py            # User Simulator implementation
â”‚   â”‚   â”œâ”€â”€ pipeline.py        # dialogue pipeline
â”‚   â”‚   â”œâ”€â”€ checkpoint.py      # checkpoint management
â”‚   â”‚   â””â”€â”€ monitor.py         # dialogue monitor
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ mcp_wrapper.py     # MCP Server wrapper
â”‚   â”‚   â””â”€â”€ datasets/
â”‚   â”‚       â”œâ”€â”€ single domain/    # single-domain MCP servers & data
â”‚   â”‚       â””â”€â”€ cross domain/     # cross-domain MCP servers & data
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ cross_domain.py    # cross-domain combination discovery
â””â”€â”€ outputs/                   # output directory
```

## ðŸš€ Quickstart

### Install dependencies

```bash
pip install -r requirements.txt
```

### Option 1: run with manual configuration

Use `run.py` together with a YAML config for fine-grained control:

```bash
# basic run
python run.py --config configs/example_new.yml

# resume from checkpoints
python run.py --config configs/example_new.yml --resume

# only process a specific dataset
python run.py --config configs/example_new.yml --dataset CustomerService

# verbose output
python run.py --config configs/example_new.yml --verbose
```

### Option 2: batch processing (recommended)

In practice, you will mainly use the batch scripts:

#### Single Domain

```bash
# process all single-domain datasets
python scripts/batch_rollout_single.py --all --max-workers 32 --output-dir ./outputs_cross_xxx

# process a specific domain
python scripts/batch_rollout_single.py --domains StudentAcademicPortal

# list all available domains
python scripts/batch_rollout_single.py --list

# customize models and parameters
python scripts/batch_rollout_single.py --all \
    --agent-model openai/deepseek-v3.2-fc \
    --user-model openai/gpt-5 \
    --max-turns 20 \
    --output-dir ./outputs_single
```

#### Cross Domain

```bash
# process all cross-domain combinations
python scripts/batch_rollout_cross.py --all --max-workers 32 --output-dir ./outputs_single_xxx

# process specified cross-domain combo (order-insensitive)
python scripts/batch_rollout_cross.py --domains StudentAcademicPortal StudentFinancialServices

# list all available cross-domain combinations
python scripts/batch_rollout_cross.py --list --verbose

# generate config files for manual review
python scripts/batch_rollout_cross.py --generate-configs --config-output-dir configs/generated
```

## ðŸ“‹ Workflow

### Cross Domain Workflow

1. **Prepare data**: put the entire `outputs` generated by the Cross Domain workflow (including MCP Servers, Databases, Queries, etc.) into the corresponding directories:

   - Single-domain queries: `rollout/tools/datasets/single domain/queries/`
   - Cross-domain queries: `rollout/tools/datasets/cross domain/queries/`
2. **Run batch scripts**:

   ```bash
   # single-domain
   python scripts/batch_rollout_single.py --all

   # cross-domain
   python scripts/batch_rollout_cross.py --all
   ```
3. **Outputs**: results are saved to the directory specified by `--output-dir`

### âš ï¸ Important: MCP Server path configuration

If the MCP Servers / Databases differ between the single-domain and cross-domain generated outputs, update the default paths in `rollout/tools/mcp_wrapper.py`:

```python
# rollout/tools/mcp_wrapper.py (lines 36-37)

# use single-domain MCP servers
DEFAULT_MCP_SERVERS_DIR = _ROLLOUT_PKG_DIR / "tools" / "datasets" / "single_domain" / "mcp_servers"
DEFAULT_TOOL_LISTS_DIR = _ROLLOUT_PKG_DIR / "tools" / "datasets" / "single_domain" / "tool_lists"

# or use cross-domain MCP servers
DEFAULT_MCP_SERVERS_DIR = _ROLLOUT_PKG_DIR / "tools" / "datasets" / "cross_domain" / "mcp_servers"
DEFAULT_TOOL_LISTS_DIR = _ROLLOUT_PKG_DIR / "tools" / "datasets" / "cross_domain" / "tool_lists"
```

## âš™ï¸ Configuration

### Main config (`configs/example_new.yml`)

```yaml
# global execution settings
max_workers: 48                    # number of worker threads
resume: false                      # whether to resume from last interruption
use_checkpoints: true              # enable checkpoints

# logging
log_level: INFO
log_file: ./logs/rollout.log

# realtime output
verbose: true                      # print realtime agent/user/tool outputs
verbose_colors: true               # use colored output

# dialogue monitoring (early stop detection)
enable_monitor: true
monitor_max_no_tool_turns: 5       # stop after N consecutive turns with no tool calls

# model registry config file
models_config_file: ./configs/models.yml

# datasets
datasets:
  CustomerService:
    path: ./queries/cross_domain/Domain1_Domain2.jsonl
    output_path: ./outputs/output.jsonl
    mcp_domain: Domain1_Domain2
    tools:
      - domain1_mcp
      - domain2_mcp
    agent:
      model: openai/deepseek-v3.2-fc
      temperature: 0.2
      enable_thinking: true
    user:
      model: openai/gpt-5
      temperature: 1.0
    max_turns: 20
    max_steps_per_turn: 10
    mode: positive
```

### Model registration (`configs/models.yml`)

Supports registering custom/internal models:

```yaml
models:
  deepseek-v3.2-fc:
    provider: openai
    api_base: http://your-api-endpoint/v1
    api_key: sk-your-api-key
    mode: chat

  gpt-5:
    provider: openai
    api_base: http://your-api-endpoint/v1
    api_key: sk-your-api-key
    mode: chat
```

## ðŸ“¦ Output format

Each result is a JSONL record like:

```json
{
  "id": "unique_trajectory_id",
  "success": true,
  "messages": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "...", "tool_calls": [...]},
    {"role": "tool", "content": "...", "tool_call_id": "..."},
    ...
  ],
  "metadata": {
    "total_turns": 5,
    "stop_reason": "user_stop",
    "tool_call_count": 8
  }
}
```

## ðŸ› ï¸ Advanced usage

### CLI arguments

#### `batch_rollout_single.py`

| Arg             | Description                    | Default                   |
| --------------- | ------------------------------ | ------------------------- |
| `--all`         | process all domains            | -                         |
| `--domains`     | domains to process             | -                         |
| `--list`        | list all available domains     | -                         |
| `--agent-model` | model used by Agent            | `openai/deepseek-v3.2-fc` |
| `--user-model`  | model used by User Simulator   | `openai/gpt-5`            |
| `--max-turns`   | max dialogue turns             | 20                        |
| `--max-workers` | number of worker threads       | 8                         |
| `--output-dir`  | output directory               | `./outputs_single_0114`   |
| `--no-resume`   | disable resume from checkpoints | -                        |
| `--quiet`       | quiet mode                     | -                         |

#### `batch_rollout_cross.py`

| Arg                 | Description                                | Default |
| ------------------- | ------------------------------------------ | ------- |
| `--all`             | process all cross-domain combinations      | -       |
| `--domains`         | domain combo to process (order-insensitive) | -      |
| `--list`            | list all available combinations            | -       |
| `--merge-queries`   | merge scattered query files                | -       |
| `--require-policy`  | only process combos that have policy files  | -       |
| `--min-domains`     | minimum number of domains                  | 2       |

## ðŸ§  DeepSeek V3.2 Reasoning support

Rollout includes optimizations for DeepSeek V3.2 Thinking/Reasoning mode.

### Reasoning content cleanup

To save tokens, at the start of each new turn (counting from the User message), Rollout automatically clears **all previous turns'** `reasoning_content` in the context:

```
Turn 1:
  User: "Help me check my balance"
  Assistant: [reasoning_content: "User wants to check balance..."] + [tool_call: check_balance]
  Tool: {"balance": 1000}
  Assistant: [reasoning_content: "Balance is 1000..."] + "Your balance is 1000"

At the start of Turn 2, the context becomes:
  User: "Help me check my balance"
  Assistant: [reasoning_content: null] + [tool_call: check_balance]  â† cleared
  Tool: {"balance": 1000}                                            â† kept
  Assistant: [reasoning_content: null] + "Your balance is 1000"      â† cleared reasoning, keep content
  User: "Now help me transfer money"                                 â† new turn starts
```

**Kept:**
- âœ… all Tool Responses (kept in full)
- âœ… each step's `content` (assistant text)
- âœ… all `tool_calls`

**Cleared:**
- âŒ all previous turns' `reasoning_content`

> **Note**: Trajectories are saved **before** cleanup, so output files contain full `reasoning_content`. Cleanup only affects the context for subsequent turns.

## ðŸ”§ Post-processing tools

### Add System Prompt and Tools

By default, Rollout-generated trajectories do not include System Prompt and Tools information. Use the following scripts to add them to output files.

#### Single Domain

```bash
python add_label_single.py
```

Config (edit in `add_label_single.py`):

```python
POLICY_ROOT = "rollout/tools/datasets/single_domain/policies"   # policy directory
TOOLS_LIST = "rollout/tools/datasets/single_domain/tool_lists"  # tools directory
OUTPUT_DIR = "outputs_single"                                   # rollout output directory
RESULT_FILE = "./mt_single_domain_tool_call_thinking.jsonl"     # processed output file
```

#### Cross Domain

```bash
python add_label_cross.py
```

Config (edit in `add_label_cross.py`):

```python
POLICY_ROOT = "rollout/tools/datasets/cross_domain/policies"     # policy directory
TOOLS_LIST = "rollout/tools/datasets/cross_domain/tool_lists"    # tools directory
OUTPUT_DIR = "outputs_cross"                                     # rollout output directory
RESULT_FILE = "./mt_cross_domain_tool_call_thinking.jsonl"       # processed output file
```

> **Note**: Cross-domain mode automatically handles domain order (e.g. `A_B_C.jsonl` can match `C_B_A.md` policy).

### Preprocessing before tokenization

`add_key.py` provides an example of preprocessing before tokenization. You can customize your pipeline by following its structure:

```python
# add_key.py example structure
for line in open("your_input_file.jsonl"):
    data = json.loads(line)
    messages = [
        {
            "role": msg.get("role", None),
            "content": msg.get("content", None),
            "reasoning_content": msg.get("reasoning_content", None),  # DeepSeek V3.2 CoT
            "tool_calls": msg.get("tool_calls", None),
            "tool_call_id": msg.get("tool_call_id", None)
        }
        for msg in data["messages"]
    ]
    new_data = {
        "messages": messages,
        "id": data.get("id", generate_id(data)),
        "data_source": "agent",        # data source tag
        "use_cot": True,               # whether to use Chain-of-Thought
        "tools": data.get("tools", None)
    }
    f.write(json.dumps(new_data, ensure_ascii=False) + "\n")
```

By mimicking this structure, you can:

- unify formats across different data sources
- add custom fields (e.g. `data_source`, `use_cot`)
- filter or transform specific fields
- merge multiple sources into a single file
