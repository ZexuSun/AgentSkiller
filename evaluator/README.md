# `evaluator/`

`evaluator/` evaluates agent rollouts by **executing golden trajectories** (TrajectoryExecutor) and scoring the result with multiple evaluators (action/environment/nl_assertions/…).

For the end-to-end flow (synthesis → rollouts → evaluation), see the root [`README.md`](../README.md).

---

## Run evaluation

```bash
python -m evaluator.run_evaluation --mode all \
  --rollouts-dir rollouts/ \
  --outputs-dir outputs/ \
  --mcp-outputs-dir outputs/ \
  --output outputs/evaluation/results.jsonl
```

---

## Common modes

```bash
# Pruning only (analyze redundant golden steps and generate pruning index)
python -m evaluator.run_evaluation --mode prune

# Run all evaluators (skip pruning)
python -m evaluator.run_evaluation --mode all --no-prune

# Run a single evaluator (example: action)
python -m evaluator.run_evaluation --mode single --evaluator action
```

---

## Inputs the evaluator expects

- **rollouts**: `rollouts/*.jsonl` (multi-turn conversations + tool calls)
- **queries**: `outputs/queries/*.jsonl` (tasks generated by synthesis)
- **mcp servers / tool lists / databases**: under `outputs/`
